# Musicality Model Description

## **项目大纲：实时钢琴演奏评估系统**

### **1. 项目概述**

构建一个基于深度学习的实时钢琴演奏评估系统，能够在演奏结束后即时提供总体评分，并在后续阶段集成自然语言大模型（如Llama 3.18B）生成个性化的文字反馈。系统将利用**PercePiano**数据集进行训练，结合先进的深度学习架构，实现多维度的演奏评估。

### **2. 数据收集与预处理**

#### **2.1 数据收集**

- **数据来源**：使用**PercePiano**数据集，包含存放于`/data/all_2rounds`目录下的所有MIDI文件及位于`/labels/total_2rounds.csv`的标签分数。
- **数据类型**：
  - **MIDI文件**：记录了钢琴演奏的详细信息，包括音符、节奏、力度等。
  - **标签数据**：由音乐专家标注的多维度感知特征评分。

#### **2.2 数据加载**

- **读取MIDI文件**：使用`pretty_midi`库解析MIDI文件，提取演奏信息。
- **读取标签数据**：使用`pandas`读取CSV文件，获取每个MIDI文件对应的评分。

#### **2.3 数据清洗与验证**

- **文件存在性检查**：确保每个标签数据对应的MIDI文件存在，处理缺失文件的情况。
- **数据一致性**：验证标签数据与MIDI文件的一致性，确保评分准确对应。

### **3. 特征提取**

#### **3.1 低层次特征**

- **节奏特征（Rhythm）**：
  - **算法**：使用`librosa`的节拍追踪算法（如`beat_track`）估计演奏的节奏和速度。
  - **提取内容**：平均节拍速度（BPM）、节拍稳定性等。

- **音高特征（Pitch）**：
  - **算法**：采用`pYin`算法进行F0估计，提取音高轮廓。
  - **转换与归一化**：
    - 将F0值转换为MIDI音高（公式：`m = 69 + 12 * log2(f / 440 Hz)`）。
    - 归一化到0到1的范围，确保不同音高的统一尺度。

- **力度特征（Dynamics）**：
  - **提取方法**：计算音符的平均力度（Velocity），反映演奏的力度变化。

#### **3.2 高层次特征**

- **表达与情感特征（Expressiveness & Emotion）**：
  - **提取内容**：动态变化、音色变化、节奏灵活性等，反映演奏的情感表达。
  - **算法**：结合音高和力度变化，利用统计方法或深度学习自动提取相关特征。

#### **3.3 特征向量化**

- **统一特征向量**：将所有提取的特征按固定顺序排列，形成统一的特征向量，作为模型的输入。

### **4. 模型设计与训练**

#### **4.1 模型选择**

参考论文中的模型架构，选择并设计适合的深度神经网络（DNN）架构：

- **全卷积神经网络（FCN）**：
  - **用途**：处理高层次的音高轮廓特征，捕捉演奏中的细微变化。
  - **结构**：多层1D卷积、批归一化、ReLU激活、平均池化，最终通过全连接层输出评分。

- **卷积循环神经网络（CRNN）**：
  - **用途**：处理低层次的梅尔频谱特征，捕捉时序依赖。
  - **结构**：多层2D卷积、批归一化、ELU激活、最大池化，后接GRU层和全连接层输出评分。

- **混合模型（Hybrid Model）**：
  - **用途**：综合利用FCN和CRNN的优势，处理多模态特征。
  - **结构**：并行处理PC-FCN和M-CRNN的输出，通过全连接层融合特征，输出最终评分。

#### **4.2 模型训练**

- **损失函数**：使用均方误差（MSE）作为回归任务的损失函数，衡量预测评分与真实评分的差距。
- **优化器**：采用Adam优化算法，具有自适应学习率，适合处理大规模参数优化。
- **训练策略**：
  - **数据分割**：将数据集划分为训练集（80%）、验证集（10%）和测试集（10%）。
  - **批量训练**：使用小批量梯度下降，加速训练过程并稳定收敛。
  - **早停机制**：监控验证集的损失，防止过拟合，当验证损失不再下降时提前停止训练。

#### **4.3 数据增强**

- **MIDI数据增强**：
  - **音高移位**：随机调整音高，生成多样化的训练样本。
  - **速度变化**：调整演奏速度，模拟不同的演奏节奏。
  - **力度调整**：随机改变力度，增加模型对力度变化的鲁棒性。

### **5. 实时音频处理与评分**

#### **5.1 实时音频捕获**

- **工具**：使用`sounddevice`库进行实时音频捕获，获取演奏者的音频输入。
- **配置**：
  - **采样率**：设置为44,100 Hz，保证音频质量。
  - **通道数**：单声道处理，简化计算。
  - **块大小**：设置合适的块持续时间（如2秒），平衡实时性和计算负担。

#### **5.2 实时特征提取**

- **音高检测**：
  - **算法**：使用`pYin`进行实时F0估计，提取音高轮廓。
  - **优化**：针对实时处理，优化算法参数，减少计算延迟。

- **节奏与力度提取**：
  - **节奏**：实时估计节拍速度和节奏稳定性。
  - **力度**：计算实时音频块的平均力度，反映演奏力度变化。

#### **5.3 实时模型推理**

- **模型加载**：将预训练的DNN模型加载到设备（GPU或CPU）。
- **输入格式化**：将实时提取的特征向量化，调整为模型所需的输入形状。
- **推理过程**：进行前向传播，生成实时评分。
- **评分累积**：将每个音频块的评分累积，等待演奏结束后计算总体评分。

### **6. 总体评分输出**

#### **6.1 评分聚合**

- **方法**：对所有实时块的评分进行平均或加权平均，得到演奏的总体评分。
- **加权策略**：根据不同特征的重要性，赋予不同权重，提高评分的准确性。

#### **6.2 评分标准**

- **多维度评分**：不仅提供总体评分，还独立展示各个感知特征（如节奏、音高、力度）的评分，便于细化分析。

### **7. 个性化文字反馈生成**

#### **7.1 集成自然语言大模型**

- **模型选择**：使用Llama 3.18B或类似的高级自然语言生成模型，通过API调用进行集成。
- **API配置**：配置OpenAI API或其他可用的接口，确保安全性和效率。

#### **7.2 反馈生成流程**

- **评分与特征分析**：将模型生成的评分和特征分析结果作为输入，构建详细的反馈提示（prompt）。
- **生成策略**：
  - **具体化**：根据不同评分，生成具体的改进建议，如节奏稳定性、音高准确性等。
  - **个性化**：根据演奏者的具体表现，提供有针对性的反馈，促进其技能提升。

#### **7.3 示例反馈**

- **高分反馈**：强调演奏的优点，鼓励继续保持。
- **低分反馈**：指出具体不足，提供改进建议。
- **综合反馈**：结合多个评分维度，给出全面的演奏评价。

### **8. 部署与优化**

#### **8.1 模型优化**

- **模型压缩**：使用剪枝、量化等技术减少模型参数，提高推理速度。
- **并行计算**：利用GPU加速实时特征提取和模型推理，降低延迟。

#### **8.2 系统架构**

- **模块化设计**：将系统划分为独立模块（数据处理、模型推理、反馈生成），便于维护和升级。
- **实时性能**：优化音频捕获、特征提取和模型推理流程，确保系统在实际使用中的实时性和稳定性。

#### **8.3 部署环境**

- **硬件选择**：选择高性能计算设备（如带GPU的服务器）或边缘设备（如Raspberry Pi）进行部署，根据需求权衡性能和成本。
- **软件环境**：配置适当的操作系统和依赖库，确保系统的兼容性和可扩展性。

### **9. 工具与技术**

#### **9.1 编程语言与框架**

- **Python**：主要编程语言，因其丰富的音频处理和深度学习库。
- **深度学习框架**：使用**PyTorch**，因其灵活性和广泛的社区支持。

#### **9.2 音频处理库**

- **Librosa**：用于音频特征提取，如节奏估计、F0检测。
- **PrettyMIDI**：用于解析和处理MIDI文件。
- **SoundDevice**：用于实时音频捕获和播放。

#### **9.3 数据处理与分析**

- **Pandas**：用于处理标签数据和数据分析。
- **NumPy**：用于数值计算和数组操作。
- **Matplotlib**：用于数据可视化，如绘制训练曲线、特征分布。

#### **9.4 自然语言处理**

- **OpenAI API**：用于调用Llama 3.18B或类似模型生成文字反馈。
- **Hugging Face Transformers**：可选，用于扩展自然语言处理功能。

### **10. 未来改进与扩展**

#### **10.1 模型架构优化**

- **Transformer模型**：探索基于Transformer的架构，利用自注意力机制提升对长期依赖关系的捕捉能力。
- **注意力机制**：在现有模型中引入注意力层，增强模型对重要特征的关注。

#### **10.2 数据增强与扩展**

- **多样化数据增强**：实施更多样化的数据增强策略，如随机移调、速度变化、力度调整等，提升模型的鲁棒性。
- **扩展数据集**：增加更多演奏者、不同风格和难度级别的演奏数据，增强模型的泛化能力。

#### **10.3 多模态信息集成**

- **视觉信息**：结合视频数据（如演奏者的手部动作、姿态）与音频特征，提升评估的全面性和准确性。
- **多模态深度学习模型**：设计融合音频和视频特征的深度学习架构，实现更丰富的演奏评估。

#### **10.4 模型解释与可视化**

- **显著性图与Grad-CAM**：使用显著性图、Grad-CAM等技术，理解模型对哪些特征区域更加敏感，提升模型的可解释性。
- **特征重要性分析**：分析不同特征对评分的贡献，优化特征提取和模型训练流程。

#### **10.5 用户界面与体验**

- **图形用户界面（GUI）**：开发友好的用户界面，方便用户进行演奏录制、实时评分查看和反馈阅读。
- **移动端应用**：考虑开发移动端应用，实现便携式的实时演奏评估和反馈获取。

#### **10.6 实时性能优化**

- **低延迟设计**：优化音频捕获、特征提取和模型推理流程，确保系统在实际使用中的实时性和稳定性。
- **边缘计算部署**：探索在边缘设备（如Raspberry Pi）上的部署，提升系统的便携性和响应速度。

---

## **详细模块介绍**

### **1. 数据加载与预处理**

#### **1.1 读取与解析MIDI文件**

- **工具与库**：使用`pretty_midi`库解析MIDI文件，提取音符信息（如起始时间、结束时间、音高、力度等）。
- **操作步骤**：
  - **加载MIDI文件**：读取并解析MIDI文件，获取各个音轨和音符的详细信息。
  - **提取音高与力度**：遍历音轨，提取所有非打击乐器的音符音高和力度信息。

#### **1.2 读取标签数据**

- **工具与库**：使用`pandas`读取CSV格式的标签文件。
- **操作步骤**：
  - **加载CSV文件**：读取包含每个MIDI文件对应的感知特征评分的数据表。
  - **数据匹配**：确保每个MIDI文件都有对应的评分数据，处理缺失或不匹配的情况。

#### **1.3 数据清洗与验证**

- **一致性检查**：验证MIDI文件和标签数据的一一对应关系，确保数据完整性。
- **异常处理**：处理解析MIDI文件时可能出现的错误，如文件损坏、不符合格式等。

### **2. 特征提取**

#### **2.1 节奏特征提取**

- **算法**：采用`librosa`库中的节拍追踪算法（如`librosa.beat.beat_track`）估计演奏的节奏和速度。
- **提取内容**：
  - **节拍速度（BPM）**：估计演奏的节拍速度。
  - **节奏稳定性**：分析节奏的一致性和稳定性，反映演奏的节奏控制能力。

#### **2.2 音高特征提取**

- **算法**：使用`pYin`算法进行F0估计，提取音高轮廓。
- **转换与归一化**：
  - **F0到MIDI转换**：使用公式`m = 69 + 12 * log2(f / 440 Hz)`将F0值转换为MIDI音高。
  - **归一化**：将MIDI音高归一化到0到1的范围，以统一不同音高的尺度。

#### **2.3 力度特征提取**

- **提取方法**：计算音符的平均力度（Velocity），反映演奏的力度变化和控制能力。

#### **2.4 高层次特征提取**

- **表达与情感特征**：
  - **动态变化**：分析力度和音高的变化模式，反映演奏的动态控制和情感表达。
  - **节奏灵活性**：评估演奏者对节奏的掌控和变化能力。

#### **2.5 特征向量化**

- **统一特征向量**：将所有提取的特征按照预定义的顺序排列，形成固定长度的特征向量，作为深度学习模型的输入。

### **3. 模型设计与训练**

#### **3.1 模型架构选择**

- **全卷积神经网络（FCN）**：
  - **适用场景**：处理高层次的音高轮廓特征，捕捉演奏中的细微变化。
  - **结构特点**：
    - 多层1D卷积层，捕捉音高轮廓中的局部特征。
    - 批归一化和ReLU激活，提升训练稳定性和非线性表达能力。
    - 平均池化层，汇聚特征信息，输出固定长度的特征向量。
    - 全连接层，生成最终的评分输出。

- **卷积循环神经网络（CRNN）**：
  - **适用场景**：处理低层次的梅尔频谱特征，捕捉时序依赖和动态变化。
  - **结构特点**：
    - 多层2D卷积层，提取时频域的局部特征。
    - 批归一化和ELU激活，增强模型的表达能力。
    - 最大池化层，降低特征维度，突出重要特征。
    - GRU层，捕捉时序依赖关系，处理动态变化。
    - 全连接层，生成最终的评分输出。

- **混合模型（Hybrid Model）**：
  - **适用场景**：综合利用FCN和CRNN的优势，处理多模态特征。
  - **结构特点**：
    - 并行处理PC-FCN和M-CRNN的输出，分别处理高层次和低层次特征。
    - 特征融合，通过全连接层将两部分特征结合，生成综合评分输出。

#### **3.2 模型训练流程**

- **数据分割**：
  - **训练集**：80%的数据，用于模型参数学习。
  - **验证集**：10%的数据，用于模型性能评估和超参数调优。
  - **测试集**：10%的数据，用于最终模型性能评估。

- **损失函数与优化器**：
  - **损失函数**：均方误差（MSE），衡量预测评分与真实评分之间的差距。
  - **优化器**：Adam优化算法，具备自适应学习率，适合处理大规模参数优化。

- **训练策略**：
  - **批量训练**：使用小批量梯度下降，提升训练效率和稳定性。
  - **早停机制**：监控验证集的损失，防止过拟合，提前停止训练以保持模型的泛化能力。

#### **3.3 模型评估**

- **评估指标**：
  - **R²（决定系数）**：衡量模型解释数据变异性的能力。
  - **Pearson相关系数（ρ）**：衡量模型预测值与真实值之间的线性相关性。

- **模型选择**：根据验证集和测试集的评估结果，选择表现最佳的模型架构。

### **4. 实时音频处理与评分**

#### **4.1 实时音频捕获**

- **工具与库**：使用`sounddevice`库进行实时音频捕获，获取演奏者的音频输入。
- **配置参数**：
  - **采样率**：44,100 Hz，保证音频质量。
  - **通道数**：单声道，简化计算和特征提取。
  - **块大小**：设置合理的块持续时间（如2秒），在实时性和计算负担之间找到平衡。

#### **4.2 实时特征提取**

- **节奏与音高检测**：
  - **节奏**：实时估计节拍速度和节奏稳定性，使用`librosa.beat.beat_track`。
  - **音高**：使用`pYin`算法进行实时F0估计，提取音高轮廓，转换为MIDI音高并归一化。
- **力度提取**：
  - **方法**：计算实时音频块的平均力度，反映演奏的力度变化。

#### **4.3 实时模型推理**

- **模型加载**：加载预训练的DNN模型（如FCN、CRNN或混合模型），部署到设备（GPU或CPU）。
- **输入格式化**：将实时提取的特征向量化，调整为模型所需的输入形状。
- **推理过程**：进行前向传播，生成实时评分，并累积评分结果。

### **5. 总体评分输出**

#### **5.1 评分聚合方法**

- **平均评分**：对所有实时块的评分进行简单平均，得到演奏的总体评分。
- **加权平均**：根据不同特征的重要性，赋予不同权重，计算加权平均评分，提高评分的准确性。

#### **5.2 多维度评分展示**

- **独立评分**：不仅提供总体评分，还独立展示各个感知特征（如节奏、音高、力度）的评分，便于细化分析和改进。
- **综合报告**：生成详细的评分报告，包含各个维度的评分及其分析，辅助演奏者理解自己的表现。

### **6. 个性化文字反馈生成**

#### **6.1 集成自然语言大模型**

- **模型选择**：使用Llama 3.18B或类似的高级自然语言生成模型，通过API调用进行集成（如OpenAI API）。
- **API配置**：配置API密钥和接口，确保安全性和高效性。

#### **6.2 反馈生成流程**

- **评分与特征分析**：将模型生成的评分和特征分析结果作为输入，构建详细的反馈提示（prompt）。
- **生成策略**：
  - **具体化**：根据不同评分，生成具体的改进建议，如节奏稳定性、音高准确性等。
  - **个性化**：根据演奏者的具体表现，提供有针对性的反馈，促进其技能提升。

#### **6.3 反馈内容设计**

- **高分反馈**：强调演奏的优点，鼓励继续保持。
- **低分反馈**：指出具体不足，提供改进建议。
- **综合反馈**：结合多个评分维度，给出全面的演奏评价。

### **7. 部署与优化**

#### **7.1 模型优化**

- **模型压缩**：应用剪枝、量化等技术减少模型参数，提高推理速度，降低计算资源需求。
- **并行计算**：利用GPU加速实时特征提取和模型推理，确保低延迟。

#### **7.2 系统架构设计**

- **模块化设计**：将系统划分为独立模块（数据处理、模型推理、反馈生成），便于维护和升级。
- **实时性能**：
  - **优化步骤**：优化音频捕获、特征提取和模型推理流程，确保系统在实际使用中的实时性和稳定性。
  - **资源管理**：合理分配计算资源，避免资源争用导致的性能瓶颈。

#### **7.3 部署环境**

- **硬件选择**：
  - **高性能服务器**：适用于需要高计算能力和低延迟的实时评估。
  - **边缘设备**：如Raspberry Pi，适用于便携式应用，但需优化模型以适应有限的计算资源。

- **软件环境**：
  - **操作系统**：选择稳定的操作系统，如Ubuntu或其他Linux发行版，支持必要的音频和深度学习库。
  - **依赖管理**：使用虚拟环境（如`venv`或`conda`）管理项目依赖，确保环境的一致性和可移植性。

### **8. 工具与技术**

#### **8.1 编程语言与框架**

- **Python**：主要编程语言，因其丰富的音频处理和深度学习库。
- **深度学习框架**：使用**PyTorch**，因其灵活性和广泛的社区支持。

#### **8.2 音频处理库**

- **Librosa**：用于音频特征提取，如节奏估计、F0检测。
- **PrettyMIDI**：用于解析和处理MIDI文件。
- **SoundDevice**：用于实时音频捕获和播放。

#### **8.3 数据处理与分析**

- **Pandas**：用于处理标签数据和数据分析。
- **NumPy**：用于数值计算和数组操作。
- **Matplotlib**：用于数据可视化，如绘制训练曲线、特征分布。

#### **8.4 自然语言处理**

- **OpenAI API**：用于调用Llama 3.18B或类似模型生成文字反馈。
- **Hugging Face Transformers**：可选，用于扩展自然语言处理功能。

### **9. 未来改进与扩展**

#### **9.1 模型架构优化**

- **Transformer模型**：
  - **目标**：利用自注意力机制提升对长期依赖关系的捕捉能力，增强模型的表达力。
  - **实现**：设计基于Transformer的模型架构，替代传统的LSTM或GRU层，提升模型性能。

- **注意力机制**：
  - **目标**：在现有模型中引入注意力层，增强模型对重要特征的关注。
  - **实现**：在卷积层或循环层之后添加注意力机制，优化特征聚合过程。

#### **9.2 数据增强与扩展**

- **多样化数据增强**：
  - **方法**：实施更多样化的数据增强策略，如随机移调、速度变化、力度调整等，提升模型的鲁棒性。
  - **工具**：使用`pretty_midi`库或其他MIDI处理工具进行数据增强。

- **扩展数据集**：
  - **方法**：增加更多演奏者、不同风格和难度级别的演奏数据，增强模型的泛化能力。
  - **挑战**：确保新数据的质量和标注的一致性，避免引入偏差。

#### **9.3 多模态信息集成**

- **视觉信息**：
  - **目标**：结合视频数据（如演奏者的手部动作、姿态）与音频特征，提升评估的全面性和准确性。
  - **工具与库**：使用OpenCV或其他计算机视觉库处理视频数据，提取手部动作和姿态特征。

- **多模态深度学习模型**：
  - **目标**：设计融合音频和视频特征的深度学习架构，实现更丰富的演奏评估。
  - **实现**：构建包含音频特征提取模块和视频特征提取模块的多模态模型，通过特征融合层结合多模态信息。

#### **9.4 模型解释与可视化**

- **显著性图与Grad-CAM**：
  - **目标**：理解模型对哪些特征区域更加敏感，提升模型的可解释性。
  - **工具与方法**：使用PyTorch的自动微分功能，计算梯度并生成显著性图，如Grad-CAM，用于可视化模型关注的特征区域。

- **特征重要性分析**：
  - **目标**：分析不同特征对评分的贡献，优化特征提取和模型训练流程。
  - **方法**：使用特征重要性评估方法，如SHAP值或特征扰动，量化各特征对模型预测的影响。

#### **9.5 用户界面与体验**

- **图形用户界面（GUI）**：
  - **目标**：开发友好的用户界面，方便用户进行演奏录制、实时评分查看和反馈阅读。
  - **工具与库**：使用PyQt或Tkinter等GUI框架构建桌面应用，或使用Web框架（如Flask、Django）构建Web应用。

- **移动端应用**：
  - **目标**：实现便携式的实时演奏评估和反馈获取，提升用户体验。
  - **工具与框架**：使用跨平台移动开发框架（如React Native、Flutter），集成音频捕获和模型推理功能。

#### **9.6 实时性能优化**

- **低延迟设计**：
  - **目标**：优化音频捕获、特征提取和模型推理流程，确保系统在实际使用中的实时性和稳定性。
  - **方法**：
    - **并行处理**：采用多线程或异步编程技术，实现音频捕获、特征提取和模型推理的并行处理。
    - **优化算法**：优化特征提取算法参数，减少计算时间，提升实时性。

- **边缘计算部署**：
  - **目标**：在边缘设备（如Raspberry Pi）上部署模型，提升系统的便携性和响应速度。
  - **方法**：
    - **模型量化**：将模型转换为低精度格式（如INT8），减少模型大小和计算需求。
    - **优化框架**：使用轻量级深度学习框架（如TensorFlow Lite、ONNX Runtime）进行模型部署，提升推理速度。

### **10. 工具与技术**

#### **10.1 编程语言与框架**

- **Python**：
  - **用途**：主要编程语言，适用于数据处理、特征提取、模型构建与训练。
  - **优势**：丰富的音频处理和深度学习库，广泛的社区支持。

- **深度学习框架（PyTorch）**：
  - **用途**：构建和训练深度神经网络模型。
  - **优势**：动态计算图，易于调试和扩展，支持GPU加速。

#### **10.2 音频处理库**

- **Librosa**：
  - **用途**：音频特征提取，如节奏估计、F0检测、梅尔频谱计算。
  - **优势**：功能丰富，易于使用，广泛应用于音频分析领域。

- **PrettyMIDI**：
  - **用途**：解析和处理MIDI文件，提取音符信息。
  - **优势**：高效的MIDI数据处理能力，支持复杂的MIDI操作。

- **SoundDevice**：
  - **用途**：实时音频捕获和播放，支持跨平台音频流处理。
  - **优势**：低延迟、高性能，易于集成到实时应用中。

#### **10.3 数据处理与分析**

- **Pandas**：
  - **用途**：处理标签数据和数据分析，提供高效的数据结构和数据操作功能。
  - **优势**：高效的数据读取、清洗和处理能力，适用于大规模数据集。

- **NumPy**：
  - **用途**：数值计算和数组操作，支持高性能的多维数组计算。
  - **优势**：高效的数值计算能力，与深度学习框架无缝集成。

- **Matplotlib**：
  - **用途**：数据可视化，如绘制训练曲线、特征分布和模型评估结果。
  - **优势**：功能强大，支持多种类型的图表和自定义。

#### **10.4 自然语言处理**

- **OpenAI API**：
  - **用途**：调用Llama 3.18B或类似模型生成文字反馈，支持多种语言和自定义提示。
  - **优势**：高质量的自然语言生成能力，易于集成到现有系统中。

- **Hugging Face Transformers**：
  - **用途**：扩展自然语言处理功能，支持更多预训练模型和自定义模型部署。
  - **优势**：丰富的预训练模型库，灵活的模型加载和微调能力。

---

## **总结**

通过上述详细的大纲，您可以系统性地构建一个基于**PercePiano**数据集的实时钢琴演奏评估系统。该系统涵盖了从数据加载与预处理、特征提取、深度学习模型设计与训练，到实时音频处理、总体评分输出以及个性化文字反馈生成的完整流程。随着项目的推进，您可以根据实际需求和研究发现，进一步优化和扩展系统功能，提升模型的准确性和用户体验。

### **关键步骤回顾**

1. **数据加载与预处理**：确保数据的完整性和一致性，提取必要的特征。
2. **特征提取**：从MIDI文件中提取低层次和高层次的感知特征，形成统一的特征向量。
3. **模型设计与训练**：选择合适的深度学习架构，进行模型训练和评估，确保模型的泛化能力。
4. **实时音频处理与评分**：实现实时音频捕获、特征提取和模型推理，生成实时评分。
5. **总体评分输出**：聚合实时评分，提供全面的演奏评价。
6. **个性化文字反馈生成**：利用自然语言大模型生成详细的、个性化的改进建议，提升用户体验。
7. **部署与优化**：优化模型和系统架构，确保系统的实时性和稳定性，便于实际应用。
8. **工具与技术**：合理选择和使用编程语言、框架和库，提升开发效率和系统性能。
9. **未来改进与扩展**：持续优化和扩展系统功能，提升模型性能和用户体验。

## Fine-tuning on Music Data

This section describes how to fine-tune the QwQ-32B model on the MusicPile dataset to enhance its musical knowledge.

### Requirements

- QwQ-32B model (local path: `F:\huggingface_cache\models--Qwen--QwQ-32B\snapshots\976055f8c83f394f35dbd3ab09a285a984907bd0`)
- GPU with at least 24GB VRAM (e.g., NVIDIA RTX 4090)
- Python 3.8+ with PyTorch 2.0+
- Required packages (install via `pip install -r requirements.txt`)

### Fine-tuning Process

The fine-tuning uses QLoRA (Quantized Low-Rank Adaptation) with the following key techniques:

1. **4-bit Quantization**: Reduces VRAM requirements by quantizing the base model to 4-bit precision
2. **LoRA**: Adds trainable adapter modules to specific layers while keeping most parameters frozen
3. **Efficient Training**: Uses gradient checkpointing, 8-bit optimizers, and gradient accumulation

### Dataset

The [MusicPile dataset](https://huggingface.co/datasets/m-a-p/MusicPile) contains 5.86M samples of music-related text content:

- Musical theory explanations
- Music history and analysis
- Musical notation (ABC notation)
- Composer biographies and analyses
- Sheet music descriptions

### Usage Instructions

1. **Create a subset for testing** (optional):
   ```bash
   python create_quantized_subset.py
   ```

2. **Run the fine-tuning process**:
   ```bash
   run_finetune.bat
   ```
   
   Or manually:
   ```bash
   python music_model_finetune.py
   ```

3. **Test the fine-tuned model**:
   ```bash
   run_test.bat
   ```
   
   Or manually:
   ```bash
   python test_finetune_model.py
   ```

### Configuration Options

The main fine-tuning script (`music_model_finetune.py`) can be configured by modifying:

- `MODEL_PATH`: Path to the base QwQ-32B model
- `OUTPUT_DIR`: Directory to save the fine-tuned model
- Training parameters in `TrainingArguments`
- LoRA configuration in `LoraConfig`

### Memory Optimization

Fine-tuning a 32B parameter model on limited VRAM (24GB) requires several memory optimization techniques:

- INT4 quantization via BitsAndBytes
- LoRA with trainable parameters reduced to ~0.1% of the original model
- Gradient checkpointing
- Memory-efficient optimizers (8-bit AdamW)
- Small batch sizes with gradient accumulation

### Example Results

The fine-tuned model demonstrates enhanced capabilities in:

- Music theory explanations
- Classical music analysis
- Instrumental technique descriptions
- Composer style analysis
- Musical form understanding
